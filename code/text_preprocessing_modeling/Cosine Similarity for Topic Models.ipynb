{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import nltk\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/robertpagano/metis_data/project_4/text_dataframes/clustering/df_lsa_model_20.pickle', 'rb') as f:\n",
    "    df_lsa_model_20 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process2(text):\n",
    "    \n",
    "    # lowercase\n",
    "    text=text.lower()\n",
    "    \n",
    "    #remove tags\n",
    "    text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text)\n",
    "    \n",
    "    # remove special characters and digits\n",
    "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "    \n",
    "    #lemmatize with Spacy\n",
    "    doc = nlp(text)\n",
    "    text = \" \".join([token.lemma_ for token in doc])\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english', token_pattern=\"\\\\b[a-z][a-z]+\\\\b\", ngram_range=(1,2), sublinear_tf=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_blurb_2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-1cfebbe85d28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbag_of_words_4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_blurb_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X_blurb_2' is not defined"
     ]
    }
   ],
   "source": [
    "bag_of_words_4 = vectorizer.fit_transform(X_blurb_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa = TruncatedSVD(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_id</th>\n",
       "      <th>topic_blurb</th>\n",
       "      <th>topic_0</th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>topic_5</th>\n",
       "      <th>topic_6</th>\n",
       "      <th>topic_7</th>\n",
       "      <th>...</th>\n",
       "      <th>topic_10</th>\n",
       "      <th>topic_11</th>\n",
       "      <th>topic_12</th>\n",
       "      <th>topic_13</th>\n",
       "      <th>topic_14</th>\n",
       "      <th>topic_15</th>\n",
       "      <th>topic_16</th>\n",
       "      <th>topic_17</th>\n",
       "      <th>topic_18</th>\n",
       "      <th>topic_19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>269726791</td>\n",
       "      <td>An artistic professional pack of playing cards...</td>\n",
       "      <td>0.118765</td>\n",
       "      <td>0.182466</td>\n",
       "      <td>-0.037672</td>\n",
       "      <td>0.039376</td>\n",
       "      <td>-0.048429</td>\n",
       "      <td>0.045518</td>\n",
       "      <td>0.009710</td>\n",
       "      <td>0.012686</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014650</td>\n",
       "      <td>0.027404</td>\n",
       "      <td>0.020749</td>\n",
       "      <td>0.015065</td>\n",
       "      <td>0.015533</td>\n",
       "      <td>-0.002469</td>\n",
       "      <td>0.010655</td>\n",
       "      <td>0.004937</td>\n",
       "      <td>-0.011222</td>\n",
       "      <td>0.014631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1697621882</td>\n",
       "      <td>Make it rain pancakes in FAT STACKS. \\nCreate ...</td>\n",
       "      <td>0.052765</td>\n",
       "      <td>-0.033622</td>\n",
       "      <td>-0.045537</td>\n",
       "      <td>-0.032304</td>\n",
       "      <td>-0.015172</td>\n",
       "      <td>0.015924</td>\n",
       "      <td>0.008454</td>\n",
       "      <td>-0.017053</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026820</td>\n",
       "      <td>0.029376</td>\n",
       "      <td>-0.045896</td>\n",
       "      <td>0.045269</td>\n",
       "      <td>0.014085</td>\n",
       "      <td>0.038517</td>\n",
       "      <td>-0.002483</td>\n",
       "      <td>-0.040691</td>\n",
       "      <td>-0.013559</td>\n",
       "      <td>0.030872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2046938895</td>\n",
       "      <td>Endless playability, stunning artwork and fant...</td>\n",
       "      <td>0.017984</td>\n",
       "      <td>0.001343</td>\n",
       "      <td>-0.003260</td>\n",
       "      <td>0.032234</td>\n",
       "      <td>0.002442</td>\n",
       "      <td>-0.021048</td>\n",
       "      <td>0.008277</td>\n",
       "      <td>-0.000148</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009077</td>\n",
       "      <td>-0.019254</td>\n",
       "      <td>-0.009203</td>\n",
       "      <td>0.005457</td>\n",
       "      <td>-0.005860</td>\n",
       "      <td>0.011202</td>\n",
       "      <td>-0.004034</td>\n",
       "      <td>-0.017642</td>\n",
       "      <td>-0.040061</td>\n",
       "      <td>-0.009398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1642293087</td>\n",
       "      <td>In 'Coral', use dice to make shapes in 3D! In ...</td>\n",
       "      <td>0.035804</td>\n",
       "      <td>-0.011564</td>\n",
       "      <td>-0.010287</td>\n",
       "      <td>0.004304</td>\n",
       "      <td>-0.009367</td>\n",
       "      <td>0.016175</td>\n",
       "      <td>0.008818</td>\n",
       "      <td>-0.045228</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015247</td>\n",
       "      <td>0.024286</td>\n",
       "      <td>-0.059678</td>\n",
       "      <td>0.042627</td>\n",
       "      <td>0.017888</td>\n",
       "      <td>0.007787</td>\n",
       "      <td>-0.003178</td>\n",
       "      <td>-0.007208</td>\n",
       "      <td>-0.027904</td>\n",
       "      <td>-0.051965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>932587626</td>\n",
       "      <td>Online 3D Stores for businesses that blur 3D G...</td>\n",
       "      <td>0.085045</td>\n",
       "      <td>-0.055648</td>\n",
       "      <td>-0.040125</td>\n",
       "      <td>0.035144</td>\n",
       "      <td>0.014326</td>\n",
       "      <td>-0.000312</td>\n",
       "      <td>-0.034597</td>\n",
       "      <td>-0.014971</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000892</td>\n",
       "      <td>-0.057838</td>\n",
       "      <td>0.042626</td>\n",
       "      <td>-0.003969</td>\n",
       "      <td>0.002463</td>\n",
       "      <td>0.042471</td>\n",
       "      <td>0.053249</td>\n",
       "      <td>0.005034</td>\n",
       "      <td>-0.009061</td>\n",
       "      <td>0.000517</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     topic_id                                        topic_blurb   topic_0  \\\n",
       "0   269726791  An artistic professional pack of playing cards...  0.118765   \n",
       "1  1697621882  Make it rain pancakes in FAT STACKS. \\nCreate ...  0.052765   \n",
       "2  2046938895  Endless playability, stunning artwork and fant...  0.017984   \n",
       "3  1642293087  In 'Coral', use dice to make shapes in 3D! In ...  0.035804   \n",
       "4   932587626  Online 3D Stores for businesses that blur 3D G...  0.085045   \n",
       "\n",
       "    topic_1   topic_2   topic_3   topic_4   topic_5   topic_6   topic_7  \\\n",
       "0  0.182466 -0.037672  0.039376 -0.048429  0.045518  0.009710  0.012686   \n",
       "1 -0.033622 -0.045537 -0.032304 -0.015172  0.015924  0.008454 -0.017053   \n",
       "2  0.001343 -0.003260  0.032234  0.002442 -0.021048  0.008277 -0.000148   \n",
       "3 -0.011564 -0.010287  0.004304 -0.009367  0.016175  0.008818 -0.045228   \n",
       "4 -0.055648 -0.040125  0.035144  0.014326 -0.000312 -0.034597 -0.014971   \n",
       "\n",
       "     ...     topic_10  topic_11  topic_12  topic_13  topic_14  topic_15  \\\n",
       "0    ...    -0.014650  0.027404  0.020749  0.015065  0.015533 -0.002469   \n",
       "1    ...     0.026820  0.029376 -0.045896  0.045269  0.014085  0.038517   \n",
       "2    ...     0.009077 -0.019254 -0.009203  0.005457 -0.005860  0.011202   \n",
       "3    ...    -0.015247  0.024286 -0.059678  0.042627  0.017888  0.007787   \n",
       "4    ...    -0.000892 -0.057838  0.042626 -0.003969  0.002463  0.042471   \n",
       "\n",
       "   topic_16  topic_17  topic_18  topic_19  \n",
       "0  0.010655  0.004937 -0.011222  0.014631  \n",
       "1 -0.002483 -0.040691 -0.013559  0.030872  \n",
       "2 -0.004034 -0.017642 -0.040061 -0.009398  \n",
       "3 -0.003178 -0.007208 -0.027904 -0.051965  \n",
       "4  0.053249  0.005034 -0.009061  0.000517  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lsa_model_20.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Recommender(Hp, query2, topic_blurb, top_n=10):\n",
    "    \n",
    "    cols = ['topic_0', 'topic_1', 'topic_2', 'topic_3',\n",
    "       'topic_4', 'topic_5', 'topic_6', 'topic_7', 'topic_8', 'topic_9',\n",
    "       'topic_10', 'topic_11', 'topic_12', 'topic_13', 'topic_14', 'topic_15',\n",
    "       'topic_16', 'topic_17', 'topic_18', 'topic_19']\n",
    "    \n",
    "    query = pre_process2(query2)\n",
    "    # Transform the query using the same vectorizer as above\n",
    "    doc_q2 = vectorizer.transform([query2])\n",
    "    # Use lsa model from above to transform the vectorized query\n",
    "    doc_topic_q2 = lsa.transform(doc_q2)\n",
    "\n",
    "    # Create a dataframe of query2\n",
    "    \n",
    "    Hp = Hp[cols]\n",
    "    Qp = pd.DataFrame(doc_topic_q2.round(3),\n",
    "                    index= [topic_blurb],\n",
    "                    columns = cols)\n",
    "\n",
    "    # cosine similarities\n",
    "    cos_sim = similarEntries(Hp, Qp)\n",
    "\n",
    "    # Add a new column\n",
    "    Hp['similarity']= cos_sim\n",
    "    return Hp, Qp\n",
    "\n",
    "# make a function that outputs another dataframe with a 'similarity' column added\n",
    "def similarEntries(Hp, Qp,n=3):\n",
    "    '''function returns another dataframe\n",
    "    '''\n",
    "    nrows = Hp.shape[0]\n",
    "    Hp = Hp.iloc[:,:-1] ## <- without title\n",
    "    Hp = np.asarray(Hp)\n",
    "    Qp = np.asarray(Qp)\n",
    "    \n",
    "    out = []\n",
    "    for j in range(nrows):\n",
    "        out.append(cosine_similarity(Qp.reshape(1,-1), Hp[j,:].reshape(1,-1)))\n",
    "    \n",
    "    cos_sim = out\n",
    "    \n",
    "    return [each[0][0] for each in cos_sim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "query2 = 'Endless playability, stunning artwork and fantasy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en', disable=['parser', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "TfidfVectorizer - Vocabulary wasn't fitted.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-dd0fe7e09e53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mRecommender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_lsa_model_20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'topic_blurb'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_n\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-26-6d6276eca6f9>\u001b[0m in \u001b[0;36mRecommender\u001b[0;34m(Hp, query2, topic_blurb, top_n)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpre_process2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Transform the query using the same vectorizer as above\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mdoc_q2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mquery2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Use lsa model from above to transform the vectorized query\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mdoc_topic_q2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlsa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_q2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, raw_documents, copy)\u001b[0m\n\u001b[1;32m   1629\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_tfidf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'The tfidf vector is not fitted'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1631\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1632\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m   1081\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1083\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1084\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1085\u001b[0m         \u001b[0;31m# use the same matrix-building strategy as fit_transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_check_vocabulary\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0;34m\"\"\"Check if vocabulary is empty or missing (not fit-ed)\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"%(name)s - Vocabulary wasn't fitted.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'vocabulary_'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall_or_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mattributes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 951\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFittedError\u001b[0m: TfidfVectorizer - Vocabulary wasn't fitted."
     ]
    }
   ],
   "source": [
    "Recommender(df_lsa_model_20, query2, 'topic_blurb', top_n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
